{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Title: Studio Basement for Rent – የሚከራይ ቤት\n",
      "Price: $950.00\n",
      "Date:  June 14, 2023\n",
      "Location: \n",
      "-----\n",
      "Title: Habesha Roommate Needed\n",
      "Price: $650.00\n",
      "Date:  June 3, 2023\n",
      "Location: \n",
      "-----\n",
      "Title: Newly Renovated Basement bedroom  for Rent – የሚከራይ ቤዝመንት\n",
      "Price: Not provided\n",
      "Date:  June 1, 2023\n",
      "Location: \n",
      "-----\n",
      "Title: New Basement for Rent – የሚከራይ ቤዝመንት\n",
      "Price: $1,250.00\n",
      "Date:  May 22, 2023\n",
      "Location: \n",
      "-----\n",
      "Title: One Bedroom Full Basement for Rent – ባለ አንድ መኝታ ሁሉንም ያሟላ የሚከራይ\n",
      "Price: Not provided\n",
      "Date:  May 22, 2023\n",
      "Location: \n",
      "-----\n",
      "Title: Fully furnished Room with attached private bathroom\n",
      "Price: Not provided\n",
      "Date:  April 26, 2023\n",
      "Location: \n",
      "-----\n",
      "Title: New Build Room for Rent -የሚከራይ ክፍል\n",
      "Price: $945.00\n",
      "Date:  April 19, 2023\n",
      "Location: \n",
      "-----\n",
      "Title: Full basement for Rent – የሚከራይ ቤዝመንት\n",
      "Price: Not provided\n",
      "Date:  April 17, 2023\n",
      "Location: \n",
      "-----\n",
      "Title: Room in a Basement for Rent – የሚከራይ ቤት\n",
      "Price: Not provided\n",
      "Date:  April 17, 2023\n",
      "Location: \n",
      "-----\n",
      "Title: 1 Room for Rent – Roommate Needed\n",
      "Price: Not provided\n",
      "Date:  April 10, 2023\n",
      "Location: \n",
      "-----\n",
      "Scraping page 2...\n",
      "Title: IT Training\n",
      "Price: $2,500.00\n",
      "Date:  April 8, 2023\n",
      "Location: \n",
      "-----\n",
      "Title: Room for Rent – Roommate Needed\n",
      "Price: Not provided\n",
      "Date:  April 5, 2023\n",
      "Location: \n",
      "-----\n",
      "Title: Clean Room for Rent in Basement –  የሚከራይ ክፍል\n",
      "Price: Not provided\n",
      "Date:  March 28, 2023\n",
      "Location: \n",
      "-----\n",
      "Title: Habesha Basement for Rent – የሚከራይ ቤዝመንት\n",
      "Price: Not provided\n",
      "Date:  March 25, 2023\n",
      "Location: \n",
      "-----\n",
      "Title: Room for Rent Close to National Harbor and MGM\n",
      "Price: Not provided\n",
      "Date:  March 24, 2023\n",
      "Location: \n",
      "-----\n",
      "Title: New 2 Rooms for Rent Basement የሚከራይ\n",
      "Price: Not provided\n",
      "Date:  March 24, 2023\n",
      "Location: \n",
      "-----\n",
      "Title: Full Basement & Room for Rent – የሚከራይ ቤት\n",
      "Price: Not provided\n",
      "Date:  March 22, 2023\n",
      "Location: \n",
      "-----\n",
      "Title: 2 Bedroom Full Basement – የሚከራይ ቤዝመንት Silver Spring\n",
      "Price: Not provided\n",
      "Date:  March 19, 2023\n",
      "Location: \n",
      "-----\n",
      "Title: New 3 Rooms For Rent – Very Convenient Location & Newly Refurbished\n",
      "Price: $600.00\n",
      "Date:  March 17, 2023\n",
      "Location: \n",
      "-----\n",
      "Title: 2 Rooms for rent in apartment\n",
      "Price: $700.00\n",
      "Date:  March 15, 2023\n",
      "Location: \n",
      "-----\n",
      "Scraping page 3...\n",
      "Title: Beer and Wine Business for Sale\n",
      "Price: $65,000.00\n",
      "Date:  March 14, 2023\n",
      "Location: \n",
      "-----\n",
      "Title: Habesha Babysitting – I can Babysit your child\n",
      "Price: Not provided\n",
      "Date:  March 10, 2023\n",
      "Location: \n",
      "-----\n",
      "Title: ROOM FOR RENT IN ALEXANDRIA, VIRGINIA – NEAR SPRINGFIELD MALL\n",
      "Price: Not provided\n",
      "Date:  March 9, 2023\n",
      "Location: \n",
      "-----\n",
      "Title: One Bedroom Full Basement for Rent – RENTED\n",
      "Price: Not provided\n",
      "Date:  March 8, 2023\n",
      "Location: \n",
      "-----\n",
      "Title: 1 Bedroom Apartment for Rent (walk out basement)\n",
      "Price: Not provided\n",
      "Date:  March 5, 2023\n",
      "Location: \n",
      "-----\n",
      "Title: One Room for Rent available in Basement\n",
      "Price: Not provided\n",
      "Date:  March 2, 2023\n",
      "Location: \n",
      "-----\n",
      "Title: Sunny Room for Rent\n",
      "Price: Not provided\n",
      "Date:  March 2, 2023\n",
      "Location: \n",
      "-----\n",
      "Title: Room for Rent in a Basement\n",
      "Price: $450.00\n",
      "Date:  February 28, 2023\n",
      "Location: \n",
      "-----\n",
      "Title: Room for rent in Basement\n",
      "Price: Not provided\n",
      "Date:  February 22, 2023\n",
      "Location: \n",
      "-----\n",
      "Title: Room for Rent in a Clean Two Bedroom Apartment\n",
      "Price: Not provided\n",
      "Date:  February 22, 2023\n",
      "Location: \n",
      "-----\n",
      "Scraping page 4...\n",
      "Title: Guest House in Gondar\n",
      "Price: Not provided\n",
      "Date:  February 17, 2023\n",
      "Location: \n",
      "-----\n",
      "Title: 3 Bedroom ,2 bath  Basement for Rent\n",
      "Price: Not provided\n",
      "Date:  February 11, 2023\n",
      "Location: \n",
      "-----\n",
      "Title: Room for Rent Close to DC and VA\n",
      "Price: Not provided\n",
      "Date:  February 11, 2023\n",
      "Location: \n",
      "-----\n",
      "Title: Clean and spacious Master Bedroom for Rent in a townhouse\n",
      "Price: Not provided\n",
      "Date:  February 7, 2023\n",
      "Location: \n",
      "-----\n",
      "Title: One Room for Rent in Apartment\n",
      "Price: Not provided\n",
      "Date:  February 6, 2023\n",
      "Location: \n",
      "-----\n",
      "Title: Master Bedroom with Private Bathroom – 2 Bed 2 Bath Condo\n",
      "Price: Not provided\n",
      "Date:  February 4, 2023\n",
      "Location: \n",
      "-----\n",
      "Title: Habesha Roommate Needed – 2 Bedrooms Apartment\n",
      "Price: Not provided\n",
      "Date:  January 31, 2023\n",
      "Location: \n",
      "-----\n",
      "Title: Furnished Home for Rent\n",
      "Price: Not provided\n",
      "Date:  January 24, 2023\n",
      "Location: \n",
      "-----\n",
      "Title: One Bedroom Full Basement for Rent – የሚከራይ ቤት\n",
      "Price: Not provided\n",
      "Date:  January 23, 2023\n",
      "Location: \n",
      "-----\n",
      "Title: 2 Bedrooms Full bath and Kitchen for Rent\n",
      "Price: Not provided\n",
      "Date:  January 16, 2023\n",
      "Location: \n",
      "-----\n",
      "Scraping page 5...\n",
      "Title: 2 Bedroom Basement for Rent\n",
      "Price: Not provided\n",
      "Date:  January 11, 2023\n",
      "Location: \n",
      "-----\n",
      "Title: 2 Bed rooms Available in Apartment – Roommate\n",
      "Price: Not provided\n",
      "Date:  December 22, 2022\n",
      "Location: \n",
      "-----\n",
      "Title: A Room For rent in the Apartment አንድ መኝታ ቤት የሚከራይ\n",
      "Price: Not provided\n",
      "Date:  December 17, 2022\n",
      "Location: \n",
      "-----\n",
      "Title: Apartment Room for Rent – Roommate Needed\n",
      "Price: Not provided\n",
      "Date:  December 13, 2022\n",
      "Location: \n",
      "-----\n",
      "Title: 2 Bedroom 1 Bathroom Full Kitchen Basement For Rent\n",
      "Price: Not provided\n",
      "Date:  November 11, 2022\n",
      "Location: \n",
      "-----\n",
      "Title: Room Available for Rent at White Oak, Silver Spring\n",
      "Price: Not provided\n",
      "Date:  November 9, 2022\n",
      "Location: \n",
      "-----\n",
      "Title: Basement for Rent – የሚከራይ\n",
      "Price: Not provided\n",
      "Date:  November 4, 2022\n",
      "Location: \n",
      "-----\n",
      "Title: Basement for Rent – Good Price\n",
      "Price: Not provided\n",
      "Date:  October 24, 2022\n",
      "Location: \n",
      "-----\n",
      "Title: Basement for Rent – Convenient Location\n",
      "Price: Not provided\n",
      "Date:  October 21, 2022\n",
      "Location: \n",
      "-----\n",
      "Title: Newly Renovated Basement – Private Bathrooms\n",
      "Price: $1,300.00\n",
      "Date:  October 21, 2022\n",
      "Location: \n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_page(page_number):\n",
    "    url = f\"https://mefthe.com/ads/page/{page_number}/\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    listings = soup.find_all('article', class_='listing-item')\n",
    "    for listing in listings:\n",
    "        title = listing.find('h2', class_='h4 entry-title').find('a').get_text()\n",
    "        price_element = listing.find('span', class_='post-price')\n",
    "        price = price_element.get_text().replace(\"Price \", \"\") if price_element else \"Not provided\"\n",
    "        date = listing.find('li', class_='listing-date fa-icon fa-clock-o').get_text()\n",
    "        location = listing.find('data-address')\n",
    "\n",
    "        if location:\n",
    "            location = location.get('data-address').strip()\n",
    "        else:\n",
    "            location = ''\n",
    "        \n",
    "        print(f'Title: {title}')\n",
    "        print(f'Price: {price}')\n",
    "        print(f'Date: {date}')\n",
    "        print(f'Location: {location}')\n",
    "        print('-----')\n",
    "\n",
    "for page in range(1, 6):  # change to the number of pages you want to scrape\n",
    "    print(f'Scraping page {page}...')\n",
    "    scrape_page(page)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scrapy\n",
      "  Downloading Scrapy-2.9.0-py2.py3-none-any.whl (277 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.2/277.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting Twisted>=18.9.0\n",
      "  Downloading Twisted-22.10.0-py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cryptography>=3.4.6 in /Users/ben/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from scrapy) (40.0.2)\n",
      "Collecting cssselect>=0.9.1\n",
      "  Using cached cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting itemloaders>=1.0.1\n",
      "  Downloading itemloaders-1.1.0-py3-none-any.whl (11 kB)\n",
      "Collecting parsel>=1.5.0\n",
      "  Downloading parsel-1.8.1-py2.py3-none-any.whl (17 kB)\n",
      "Collecting pyOpenSSL>=21.0.0\n",
      "  Downloading pyOpenSSL-23.2.0-py3-none-any.whl (59 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.0/59.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting queuelib>=1.4.2\n",
      "  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
      "Collecting service-identity>=18.1.0\n",
      "  Downloading service_identity-23.1.0-py3-none-any.whl (12 kB)\n",
      "Collecting w3lib>=1.17.0\n",
      "  Downloading w3lib-2.1.1-py3-none-any.whl (21 kB)\n",
      "Collecting zope.interface>=5.1.0\n",
      "  Downloading zope.interface-6.0-cp311-cp311-macosx_11_0_arm64.whl (202 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.4/202.4 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting protego>=0.1.15\n",
      "  Downloading Protego-0.2.1-py2.py3-none-any.whl (8.2 kB)\n",
      "Collecting itemadapter>=0.1.0\n",
      "  Downloading itemadapter-0.8.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: setuptools in /Users/ben/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from scrapy) (65.5.0)\n",
      "Requirement already satisfied: packaging in /Users/ben/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from scrapy) (23.1)\n",
      "Collecting tldextract\n",
      "  Downloading tldextract-3.4.4-py3-none-any.whl (93 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.3/93.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: lxml>=4.3.0 in /Users/ben/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from scrapy) (4.9.2)\n",
      "Collecting PyDispatcher>=2.0.5\n",
      "  Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/ben/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from cryptography>=3.4.6->scrapy) (1.15.1)\n",
      "Collecting jmespath>=0.9.5\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: six in /Users/ben/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from protego>=0.1.15->scrapy) (1.16.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /Users/ben/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from service-identity>=18.1.0->scrapy) (23.1.0)\n",
      "Collecting pyasn1\n",
      "  Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "Collecting pyasn1-modules\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting constantly>=15.1\n",
      "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting incremental>=21.3.0\n",
      "  Downloading incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting Automat>=0.8.0\n",
      "  Downloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting hyperlink>=17.1.1\n",
      "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.5 in /Users/ben/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from Twisted>=18.9.0->scrapy) (4.5.0)\n",
      "Requirement already satisfied: idna in /Users/ben/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from tldextract->scrapy) (3.4)\n",
      "Requirement already satisfied: requests>=2.1.0 in /Users/ben/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from tldextract->scrapy) (2.29.0)\n",
      "Collecting requests-file>=1.4\n",
      "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
      "Collecting filelock>=3.0.8\n",
      "  Downloading filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: pycparser in /Users/ben/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=3.4.6->scrapy) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ben/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from requests>=2.1.0->tldextract->scrapy) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ben/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from requests>=2.1.0->tldextract->scrapy) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ben/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from requests>=2.1.0->tldextract->scrapy) (2022.12.7)\n",
      "Installing collected packages: PyDispatcher, incremental, constantly, zope.interface, w3lib, queuelib, pyasn1, protego, jmespath, itemadapter, hyperlink, filelock, cssselect, Automat, Twisted, requests-file, pyasn1-modules, parsel, tldextract, service-identity, pyOpenSSL, itemloaders, scrapy\n",
      "Successfully installed Automat-22.10.0 PyDispatcher-2.0.7 Twisted-22.10.0 constantly-15.1.0 cssselect-1.2.0 filelock-3.12.2 hyperlink-21.0.0 incremental-22.10.0 itemadapter-0.8.0 itemloaders-1.1.0 jmespath-1.0.1 parsel-1.8.1 protego-0.2.1 pyOpenSSL-23.2.0 pyasn1-0.5.0 pyasn1-modules-0.3.0 queuelib-1.6.2 requests-file-1.5.1 scrapy-2.9.0 service-identity-23.1.0 tldextract-3.4.4 w3lib-2.1.1 zope.interface-6.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    " pip install scrapy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-16 12:26:53 [scrapy.utils.log] INFO: Scrapy 2.9.0 started (bot: scrapybot)\n",
      "2023-06-16 12:26:53 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.13, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.11.1 (main, Jan 23 2023, 16:57:30) [Clang 14.0.0 (clang-1400.0.29.202)], pyOpenSSL 23.2.0 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform macOS-14.0-arm64-arm-64bit\n",
      "2023-06-16 12:26:53 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2023-06-16 12:26:53 [py.warnings] WARNING: /Users/ben/.pyenv/versions/3.11.1/lib/python3.11/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2023-06-16 12:26:53 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2023-06-16 12:26:53 [scrapy.extensions.telnet] INFO: Telnet Password: 261ae0f7bfe5cfad\n",
      "2023-06-16 12:26:53 [py.warnings] WARNING: /Users/ben/.pyenv/versions/3.11.1/lib/python3.11/site-packages/scrapy/extensions/feedexport.py:326: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details\n",
      "  exporter = cls(crawler)\n",
      "\n",
      "2023-06-16 12:26:53 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2023-06-16 12:26:53 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2023-06-16 12:26:53 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2023-06-16 12:26:53 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2023-06-16 12:26:53 [scrapy.core.engine] INFO: Spider opened\n",
      "2023-06-16 12:26:53 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2023-06-16 12:26:53 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2023-06-16 12:26:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://mefthe.com/ads/> (referer: None)\n",
      "2023-06-16 12:26:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://mefthe.com/ads/>\n",
      "{'name': 'Studio Basement for Rent – የሚከራይ ቤት', 'price': 'Price $950.00', 'date': ' June 14, 2023'}\n",
      "2023-06-16 12:26:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://mefthe.com/ads/>\n",
      "{'name': 'Habesha Roommate Needed', 'price': 'Price $650.00', 'date': ' June 3, 2023'}\n",
      "2023-06-16 12:26:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://mefthe.com/ads/>\n",
      "{'name': 'Newly Renovated Basement bedroom  for Rent – የሚከራይ ቤዝመንት', 'price': None, 'date': ' June 1, 2023'}\n",
      "2023-06-16 12:26:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://mefthe.com/ads/>\n",
      "{'name': 'New Basement for Rent – የሚከራይ ቤዝመንት', 'price': 'Price $1,250.00', 'date': ' May 22, 2023'}\n",
      "2023-06-16 12:26:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://mefthe.com/ads/>\n",
      "{'name': 'One Bedroom Full Basement for Rent – ባለ አንድ መኝታ ሁሉንም ያሟላ የሚከራይ', 'price': None, 'date': ' May 22, 2023'}\n",
      "2023-06-16 12:26:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://mefthe.com/ads/>\n",
      "{'name': 'Fully furnished Room with attached private bathroom', 'price': None, 'date': ' April 26, 2023'}\n",
      "2023-06-16 12:26:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://mefthe.com/ads/>\n",
      "{'name': 'New Build Room for Rent -የሚከራይ ክፍል', 'price': 'Price $945.00', 'date': ' April 19, 2023'}\n",
      "2023-06-16 12:26:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://mefthe.com/ads/>\n",
      "{'name': 'Full basement for Rent – የሚከራይ ቤዝመንት', 'price': None, 'date': ' April 17, 2023'}\n",
      "2023-06-16 12:26:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://mefthe.com/ads/>\n",
      "{'name': 'Room in a Basement for Rent – የሚከራይ ቤት', 'price': None, 'date': ' April 17, 2023'}\n",
      "2023-06-16 12:26:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://mefthe.com/ads/>\n",
      "{'name': '1 Room for Rent – Roommate Needed', 'price': None, 'date': ' April 10, 2023'}\n",
      "2023-06-16 12:26:53 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2023-06-16 12:26:53 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: mefthe_ads.json\n",
      "2023-06-16 12:26:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 214,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 22729,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 0.755216,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2023, 6, 16, 16, 26, 53, 974592),\n",
      " 'httpcompression/response_bytes': 133193,\n",
      " 'httpcompression/response_count': 1,\n",
      " 'item_scraped_count': 10,\n",
      " 'log_count/DEBUG': 12,\n",
      " 'log_count/INFO': 11,\n",
      " 'log_count/WARNING': 2,\n",
      " 'memusage/max': 98648064,\n",
      " 'memusage/startup': 98648064,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2023, 6, 16, 16, 26, 53, 219376)}\n",
      "2023-06-16 12:26:53 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class MeftheSpider(scrapy.Spider):\n",
    "    name = 'mefthe_spider'\n",
    "    start_urls = ['https://mefthe.com/ads/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        AD_SELECTOR = '.listing-item'\n",
    "        for ad in response.css(AD_SELECTOR):\n",
    "            NAME_SELECTOR = 'h2 a ::text'\n",
    "            PRICE_SELECTOR = 'span.post-price ::text'\n",
    "            DATE_SELECTOR = 'li.listing-date ::text'\n",
    "            yield {\n",
    "                'name': ad.css(NAME_SELECTOR).extract_first(),\n",
    "                'price': ad.css(PRICE_SELECTOR).extract_first(),\n",
    "                'date': ad.css(DATE_SELECTOR).extract_first(),\n",
    "            }\n",
    "\n",
    "        NEXT_PAGE_SELECTOR = '.nav-links a.next ::attr(href)'\n",
    "        next_page = response.css(NEXT_PAGE_SELECTOR).extract_first()\n",
    "        if next_page:\n",
    "            yield scrapy.Request(\n",
    "                response.urljoin(next_page),\n",
    "                callback=self.parse\n",
    "            )\n",
    "\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',\n",
    "    'FEED_URI': 'mefthe_ads.json'\n",
    "})\n",
    "\n",
    "process.crawl(MeftheSpider)\n",
    "process.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-16 12:30:11 [scrapy.utils.log] INFO: Scrapy 2.9.0 started (bot: scrapybot)\n",
      "2023-06-16 12:30:11 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.13, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.11.1 (main, Jan 23 2023, 16:57:30) [Clang 14.0.0 (clang-1400.0.29.202)], pyOpenSSL 23.2.0 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform macOS-14.0-arm64-arm-64bit\n",
      "2023-06-16 12:30:11 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2023-06-16 12:30:11 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2023-06-16 12:30:11 [scrapy.extensions.telnet] INFO: Telnet Password: 8c66fea7266621c5\n",
      "2023-06-16 12:30:11 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2023-06-16 12:30:11 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2023-06-16 12:30:11 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2023-06-16 12:30:11 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2023-06-16 12:30:11 [scrapy.core.engine] INFO: Spider opened\n",
      "2023-06-16 12:30:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2023-06-16 12:30:11 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024\n"
     ]
    },
    {
     "ename": "ReactorNotRestartable",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mReactorNotRestartable\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 41\u001b[0m\n\u001b[1;32m     35\u001b[0m process \u001b[39m=\u001b[39m CrawlerProcess({\n\u001b[1;32m     36\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mFEED_FORMAT\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mjson\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     37\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mFEED_URI\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mmefthe_ads.json\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     38\u001b[0m })\n\u001b[1;32m     40\u001b[0m process\u001b[39m.\u001b[39mcrawl(MeftheSpider)\n\u001b[0;32m---> 41\u001b[0m process\u001b[39m.\u001b[39;49mstart()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/scrapy/crawler.py:390\u001b[0m, in \u001b[0;36mCrawlerProcess.start\u001b[0;34m(self, stop_after_crawl, install_signal_handlers)\u001b[0m\n\u001b[1;32m    388\u001b[0m tp\u001b[39m.\u001b[39madjustPoolsize(maxthreads\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39mgetint(\u001b[39m\"\u001b[39m\u001b[39mREACTOR_THREADPOOL_MAXSIZE\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    389\u001b[0m reactor\u001b[39m.\u001b[39maddSystemEventTrigger(\u001b[39m\"\u001b[39m\u001b[39mbefore\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mshutdown\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop)\n\u001b[0;32m--> 390\u001b[0m reactor\u001b[39m.\u001b[39;49mrun(installSignalHandlers\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/twisted/internet/base.py:1317\u001b[0m, in \u001b[0;36m_SignalReactorMixin.run\u001b[0;34m(self, installSignalHandlers)\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m, installSignalHandlers: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1317\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstartRunning(installSignalHandlers\u001b[39m=\u001b[39;49minstallSignalHandlers)\n\u001b[1;32m   1318\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmainLoop()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/twisted/internet/base.py:1299\u001b[0m, in \u001b[0;36m_SignalReactorMixin.startRunning\u001b[0;34m(self, installSignalHandlers)\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m \u001b[39mExtend the base implementation in order to remember whether signal\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[39mhandlers should be installed later.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1296\u001b[0m \u001b[39m    installed during startup.\u001b[39;00m\n\u001b[1;32m   1297\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1298\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_installSignalHandlers \u001b[39m=\u001b[39m installSignalHandlers\n\u001b[0;32m-> 1299\u001b[0m ReactorBase\u001b[39m.\u001b[39;49mstartRunning(cast(ReactorBase, \u001b[39mself\u001b[39;49m))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/twisted/internet/base.py:843\u001b[0m, in \u001b[0;36mReactorBase.startRunning\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mReactorAlreadyRunning()\n\u001b[1;32m    842\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_startedBefore:\n\u001b[0;32m--> 843\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mReactorNotRestartable()\n\u001b[1;32m    844\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_started \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    845\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stopped \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mReactorNotRestartable\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class MeftheSpider(scrapy.Spider):\n",
    "    name = 'mefthe_spider'\n",
    "    start_urls = ['https://mefthe.com/ads/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        AD_SELECTOR = '.listing-item'\n",
    "        for ad in response.css(AD_SELECTOR):\n",
    "            LINK_SELECTOR = 'h2 a ::attr(href)'\n",
    "            yield response.follow(ad.css(LINK_SELECTOR).extract_first(), self.parse_ad)\n",
    "\n",
    "        NEXT_PAGE_SELECTOR = '.nav-links a.next ::attr(href)'\n",
    "        next_page = response.css(NEXT_PAGE_SELECTOR).extract_first()\n",
    "        if next_page:\n",
    "            yield scrapy.Request(\n",
    "                response.urljoin(next_page),\n",
    "                callback=self.parse\n",
    "            )\n",
    "\n",
    "    def parse_ad(self, response):\n",
    "        NAME_SELECTOR = 'h2.entry-title ::text'\n",
    "        PRICE_SELECTOR = 'span.post-price ::text'\n",
    "        DATE_SELECTOR = 'li.listing-date ::text'\n",
    "        DESCRIPTION_SELECTOR = 'div.post-content ::text'\n",
    "        yield {\n",
    "            'name': response.css(NAME_SELECTOR).extract_first(),\n",
    "            'price': response.css(PRICE_SELECTOR).extract_first(),\n",
    "            'date': response.css(DATE_SELECTOR).extract_first(),\n",
    "            'description': response.css(DESCRIPTION_SELECTOR).extract_first(),\n",
    "            'url': response.url,\n",
    "        }\n",
    "\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',\n",
    "    'FEED_URI': 'mefthe_ads.json'\n",
    "})\n",
    "\n",
    "process.crawl(MeftheSpider)\n",
    "process.start()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
